{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d096dde-3459-47e6-8308-5ff188aa0479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_cluster_size  min_samples     score  n_clusters  noise_frac\n",
      "0                82         41.0  0.340441           2    0.352149\n",
      "1                97         48.0  0.322484           2    0.448990\n",
      "2                59          NaN  0.310227           2    0.504920\n",
      "3                59         59.0  0.310227           2    0.504920\n",
      "4                97         97.0  0.302751           2    0.644744\n",
      "5                97          NaN  0.302751           2    0.644744\n",
      "6                70         70.0  0.299722           2    0.608493\n",
      "7                70          NaN  0.299722           2    0.608493\n",
      "\n",
      "Best model: min_cluster_size= 82 min_samples= 41 n_clusters= 2 noise_frac= 0.35214914552045573\n",
      "\n",
      "Cluster summary:\n",
      "   cluster_id  size  prob_mean  prob_median  stability  bbox_min_x  \\\n",
      "0           0   664   0.945127     0.990688   0.152932  -48.649597   \n",
      "1           1   587   0.980024     1.000000   0.057115  -48.488594   \n",
      "\n",
      "   bbox_min_y  bbox_min_z  bbox_max_x  bbox_max_y  bbox_max_z  \n",
      "0   12.453988  -67.446083  -42.560169   16.746086  -63.312157  \n",
      "1   15.266109  -72.605232  -43.206501   19.724548  -65.313744  \n"
     ]
    }
   ],
   "source": [
    "# === HDBSCAN Hotspots: auto-tuned clustering → clusters + optional shells + Plotly ===\n",
    "# Input: CSV with ['middle_x','middle_y','middle_z']\n",
    "# Output:\n",
    "#   - per-gene labels + probabilities\n",
    "#   - cluster summary DataFrame (size, stability, prob stats)\n",
    "#   - optional GLB shells per cluster (KDE-on-cluster)\n",
    "#   - interactive Plotly view\n",
    "\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "try:\n",
    "    import hdbscan\n",
    "    from hdbscan.validity import validity_index as dbcv_index\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please install hdbscan: pip install hdbscan\") from e\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "\n",
    "import warnings, logging\n",
    "warnings.filterwarnings(\"ignore\")            # hide sklearn/numba/hdbscan chatter\n",
    "logging.getLogger().setLevel(logging.ERROR)  # quiet generic loggers\n",
    "\n",
    "# Config \n",
    "@dataclass\n",
    "class HDBSCANConfig:\n",
    "    scale_coords: bool = True         # standardize coords before clustering\n",
    "    min_cluster_size_strategy: str = \"auto\"  # \"auto\" or \"sqrtN\" or \"fixed\"\n",
    "    fixed_min_cluster_size: int = 20  # only used if strategy==\"fixed\"\n",
    "    min_cluster_size_grid: Tuple[float, float, int] = (0.6, 2.2, 9)\n",
    "    # grid over √N * [low..high] with 'steps' values -> tested mcs\n",
    "    min_samples_options: Tuple[Optional[float], ...] = (None, 1, 0.5, 1.0)\n",
    "    # None->equals mcs; numbers: absolute if >=2 else fraction of mcs\n",
    "    allow_single_cluster: bool = False\n",
    "    cluster_selection_method: str = \"eom\"    # \"eom\" (default) or \"leaf\"\n",
    "    metric: str = \"euclidean\"\n",
    "    # per‑cluster shells (KDE-on-cluster)\n",
    "    make_shells: bool = True\n",
    "    shell_quantiles: Tuple[float, ...] = (95, 90, 80, 60)  \n",
    "    shell_sigma_vox: float = 1.8\n",
    "    shell_grid_base: int = 96\n",
    "    shell_margin: float = 3.0\n",
    "    out_dir: Optional[str] = None\n",
    "    seed: int = 7\n",
    "\n",
    "\n",
    "# Data loading\n",
    "def load_points(csv: Optional[str]=None, df: Optional[pd.DataFrame]=None,\n",
    "                cols=(\"middle_x\",\"middle_y\",\"middle_z\")) -> np.ndarray:\n",
    "    if df is None and csv is None:\n",
    "        raise ValueError(\"Provide csv or df\")\n",
    "    if df is None:\n",
    "        df = pd.read_csv(csv)\n",
    "    pts = df[list(cols)].dropna().values.astype(np.float32)\n",
    "    if len(pts) < 10:\n",
    "        raise ValueError(f\"Too few points: {len(pts)}\")\n",
    "    return pts\n",
    "\n",
    "\n",
    "# Auto-tuning\n",
    "def _candidate_mcs(n: int, cfg: HDBSCANConfig) -> List[int]:\n",
    "    if cfg.min_cluster_size_strategy == \"fixed\":\n",
    "        return [max(5, int(cfg.fixed_min_cluster_size))]\n",
    "    base = math.sqrt(n) if cfg.min_cluster_size_strategy in (\"auto\", \"sqrtN\") else cfg.fixed_min_cluster_size\n",
    "    lo, hi, steps = cfg.min_cluster_size_grid\n",
    "    vals = np.unique(np.clip(np.rint(base * np.geomspace(lo, hi, steps)), 5, max(5, n//5)).astype(int))\n",
    "    return [int(v) for v in vals.tolist()]\n",
    "\n",
    "def _candidate_min_samples(mcs: int, options: Tuple[Optional[float], ...]) -> List[Optional[int]]:\n",
    "    out: List[Optional[int]] = []\n",
    "    for opt in options:\n",
    "        if opt is None:\n",
    "            cand = None  # special case: HDBSCAN uses mcs\n",
    "        elif opt >= 2:\n",
    "            cand = int(opt)           # absolute value\n",
    "        else:\n",
    "            cand = max(1, int(round(mcs * float(opt))))  # fraction of mcs\n",
    "        if cand not in out:           # preserve order, dedupe\n",
    "            out.append(cand)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _score_clustering(labels: np.ndarray, X: np.ndarray, clusterer: hdbscan.HDBSCAN) -> float:\n",
    "    # Components: DBCV (density-based validity), silhouette on non-noise if valid,\n",
    "    # penalty for high noise fraction, and reward for high mean membership prob.\n",
    "    labs = labels\n",
    "    n = len(labs)\n",
    "    n_clusters = len(set(labs)) - (1 if -1 in labs else 0)\n",
    "    noise_frac = np.mean(labs == -1)\n",
    "    mean_prob = float(clusterer.probabilities_[labs != -1].mean()) if np.any(labs != -1) else 0.0\n",
    "\n",
    "    # DBCV requires >= 2 clusters with non-noise\n",
    "    dbcv = 0.0\n",
    "    try:\n",
    "        if n_clusters >= 2:\n",
    "            dbcv = dbcv_index(X, labs)\n",
    "    except Exception:\n",
    "        dbcv = 0.0\n",
    "\n",
    "    sil = 0.0\n",
    "    try:\n",
    "        valid = labs != -1\n",
    "        if n_clusters >= 2 and valid.sum() > n_clusters:\n",
    "            sil = silhouette_score(X[valid], labs[valid], metric=\"euclidean\")\n",
    "    except Exception:\n",
    "        sil = 0.0\n",
    "\n",
    "    # Composite score: primary=DBCV, then silhouette, then penalties/rewards\n",
    "    score = (1.00 * dbcv) + (0.25 * sil) + (0.20 * (1.0 - noise_frac)) + (0.10 * mean_prob)\n",
    "    return float(score)\n",
    "\n",
    "def auto_hdbscan(X: np.ndarray, cfg: HDBSCANConfig):\n",
    "    # Optional scaling\n",
    "    Xs = StandardScaler().fit_transform(X) if cfg.scale_coords else X.copy()\n",
    "\n",
    "    best = None\n",
    "    tried = []\n",
    "\n",
    "    for mcs in _candidate_mcs(len(X), cfg):\n",
    "        for ms in _candidate_min_samples(mcs, cfg.min_samples_options):\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=int(mcs),\n",
    "                min_samples=None if ms is None else int(ms),\n",
    "                metric=cfg.metric,\n",
    "                cluster_selection_method=cfg.cluster_selection_method,\n",
    "                allow_single_cluster=cfg.allow_single_cluster,\n",
    "                core_dist_n_jobs=1\n",
    "            ).fit(Xs)\n",
    "            labels = clusterer.labels_\n",
    "            score = _score_clustering(labels, Xs, clusterer)\n",
    "            tried.append(dict(min_cluster_size=mcs, min_samples=ms, score=score,\n",
    "                              n_clusters=len(set(labels)) - (1 if -1 in labels else 0),\n",
    "                              noise_frac=float(np.mean(labels==-1))))\n",
    "            if best is None or score > best[\"score\"]:\n",
    "                best = dict(model=clusterer, labels=labels, score=score,\n",
    "                            min_cluster_size=mcs, min_samples=ms)\n",
    "\n",
    "    # pack diagnostics\n",
    "    diag = pd.DataFrame(tried).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    return best[\"model\"], best[\"labels\"], Xs, diag\n",
    "\n",
    "\n",
    "# Optional cluster shells \n",
    "def _grid_from_points(P: np.ndarray, grid_base=96, margin=3.0):\n",
    "    mins = P.min(0) - margin; maxs = P.max(0) + margin\n",
    "    extent = maxs - mins\n",
    "    scale = grid_base / max(float(extent.min()), 1e-6)\n",
    "    shape = np.maximum(np.round(extent * scale).astype(int), 8)\n",
    "    edges = [np.linspace(mins[i], maxs[i], int(shape[i]) + 1) for i in range(3)]\n",
    "    spacing = extent / shape\n",
    "    origin = mins\n",
    "    return edges, spacing, origin, tuple(int(s) for s in shape)\n",
    "\n",
    "def _voxelize(P: np.ndarray, edges):\n",
    "    H, _ = np.histogramdd(P, bins=edges)\n",
    "    return H.astype(np.float32)\n",
    "\n",
    "def _cluster_shells(P: np.ndarray, cfg: HDBSCANConfig):\n",
    "    \"\"\"Return {q:mesh}, {q:level} for all requested quantiles on this cluster.\"\"\"\n",
    "    if len(P) < 10:\n",
    "        return {}, {}\n",
    "    # grid\n",
    "    mins = P.min(0) - cfg.shell_margin; maxs = P.max(0) + cfg.shell_margin\n",
    "    extent = maxs - mins\n",
    "    scale = cfg.shell_grid_base / max(float(extent.min()), 1e-6)\n",
    "    shape = np.maximum(np.round(extent * scale).astype(int), 8)\n",
    "    edges = [np.linspace(mins[i], maxs[i], int(shape[i]) + 1) for i in range(3)]\n",
    "    spacing = extent / shape\n",
    "    origin = mins\n",
    "    # density\n",
    "    H, _ = np.histogramdd(P, bins=edges)\n",
    "    dens = gaussian_filter(H.astype(np.float32), sigma=cfg.shell_sigma_vox, mode=\"constant\")\n",
    "    vals = dens[dens > 0]\n",
    "    if vals.size == 0:\n",
    "        return {}, {}\n",
    "    levels = {q: float(np.percentile(vals, q)) for q in cfg.shell_quantiles}\n",
    "    # meshes\n",
    "    meshes = {}\n",
    "    for q, lvl in levels.items():\n",
    "        V, F, _, _ = marching_cubes(dens, level=lvl, spacing=spacing)\n",
    "        V = V + origin\n",
    "        meshes[q] = trimesh.Trimesh(vertices=V, faces=F, process=True)\n",
    "    return meshes, levels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# outputs and run pipeline\n",
    "@dataclass\n",
    "class HDBSCANOutputs:\n",
    "    labels: np.ndarray\n",
    "    probabilities: np.ndarray\n",
    "    clusterer: object\n",
    "    X_scaled: np.ndarray\n",
    "    tuning_diagnostics: pd.DataFrame\n",
    "    cluster_summary: pd.DataFrame\n",
    "    cluster_meshes: Dict[int, Dict[float, trimesh.Trimesh]]   # per cluster, per quantile\n",
    "    cluster_levels: Dict[int, Dict[float, float]]             # per cluster, per quantile\n",
    "\n",
    "\n",
    "def run_hdbscan_hotspots(pts: np.ndarray, cfg: HDBSCANConfig) -> HDBSCANOutputs:\n",
    "    model, labels, Xs, diag = auto_hdbscan(pts, cfg)\n",
    "\n",
    "    clusters = sorted([c for c in np.unique(labels) if c != -1])\n",
    "    probs = model.probabilities_\n",
    "    rows = []\n",
    "    meshes_all: Dict[int, Dict[float, trimesh.Trimesh]] = {}\n",
    "    levels_all: Dict[int, Dict[float, float]] = {}\n",
    "\n",
    "    for cid in clusters:\n",
    "        idx = (labels == cid)\n",
    "        P = pts[idx]\n",
    "        pr = probs[idx]\n",
    "\n",
    "        rows.append(dict(\n",
    "            cluster_id=int(cid),\n",
    "            size=int(idx.sum()),\n",
    "            prob_mean=float(pr.mean()),\n",
    "            prob_median=float(np.median(pr)),\n",
    "            stability=float(getattr(model, \"cluster_persistence_\", [None]*(cid+1))[cid]\n",
    "                            if hasattr(model, \"cluster_persistence_\") else None),\n",
    "            bbox_min_x=float(P[:,0].min()), bbox_min_y=float(P[:,1].min()), bbox_min_z=float(P[:,2].min()),\n",
    "            bbox_max_x=float(P[:,0].max()), bbox_max_y=float(P[:,1].max()), bbox_max_z=float(P[:,2].max()),\n",
    "        ))\n",
    "\n",
    "        if cfg.make_shells:\n",
    "            qmesh, qlevels = _cluster_shells(P, cfg)\n",
    "            if qmesh:\n",
    "                meshes_all[cid] = qmesh\n",
    "                levels_all[cid] = qlevels\n",
    "                if cfg.out_dir:\n",
    "                    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "                    for q, mesh in qmesh.items():\n",
    "                        mesh.export(os.path.join(cfg.out_dir, f\"cluster_{cid}_q{int(q)}.glb\"))\n",
    "\n",
    "    summary = pd.DataFrame(rows).sort_values(\"size\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return HDBSCANOutputs(\n",
    "        labels=labels,\n",
    "        probabilities=probs,\n",
    "        clusterer=model,\n",
    "        X_scaled=Xs,\n",
    "        tuning_diagnostics=diag,\n",
    "        cluster_summary=summary,\n",
    "        cluster_meshes=meshes_all,\n",
    "        cluster_levels=levels_all\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Plotly visualization\n",
    "def _scatter_points_colored(pts, labels, probs, size=2.4):\n",
    "    # map labels → colors; noise=-1 gray\n",
    "    uniq = sorted(np.unique(labels).tolist())\n",
    "    palette = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\n",
    "               \"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\"]\n",
    "    color_map = {-1: \"#bdbdbd\"}\n",
    "    ci = 0\n",
    "    for u in uniq:\n",
    "        if u == -1: \n",
    "            continue\n",
    "        color_map[u] = palette[ci % len(palette)]\n",
    "        ci += 1\n",
    "\n",
    "    traces = []\n",
    "    for u in uniq:\n",
    "        mask = (labels == u)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        if u == -1:\n",
    "            traces.append(go.Scatter3d(\n",
    "                x=pts[mask,0], y=pts[mask,1], z=pts[mask,2],\n",
    "                mode=\"markers\", name=\"noise\",\n",
    "                marker=dict(size=size, opacity=0.6, color=color_map[-1]),\n",
    "                hovertemplate=\"noise<extra></extra>\",\n",
    "            ))\n",
    "        else:\n",
    "            traces.append(go.Scatter3d(\n",
    "                x=pts[mask,0], y=pts[mask,1], z=pts[mask,2],\n",
    "                mode=\"markers\", name=f\"cluster {u}\",\n",
    "                marker=dict(size=size, opacity=0.9, color=color_map[u]),\n",
    "                # no %-formatting here; leave Plotly’s %{...} placeholders intact\n",
    "                hovertemplate=f\"cluster={u}<br>prob=%{{customdata:.2f}}<extra></extra>\",\n",
    "                customdata=probs[mask],\n",
    "            ))\n",
    "    return traces\n",
    "\n",
    "def _concat_meshes(meshes: list[trimesh.Trimesh]):\n",
    "    \"\"\"Merge multiple trimesh objects into one Mesh3d payload for Plotly.\"\"\"\n",
    "    if not meshes:\n",
    "        return None\n",
    "    xs, ys, zs, is_, js_, ks_ = [], [], [], [], [], []\n",
    "    v_offset = 0\n",
    "    for m in meshes:\n",
    "        V, F = m.vertices, m.faces\n",
    "        xs.append(V[:, 0]); ys.append(V[:, 1]); zs.append(V[:, 2])\n",
    "        is_.append(F[:, 0] + v_offset)\n",
    "        js_.append(F[:, 1] + v_offset)\n",
    "        ks_.append(F[:, 2] + v_offset)\n",
    "        v_offset += V.shape[0]\n",
    "    return dict(\n",
    "        x=np.concatenate(xs),\n",
    "        y=np.concatenate(ys),\n",
    "        z=np.concatenate(zs),\n",
    "        i=np.concatenate(is_),\n",
    "        j=np.concatenate(js_),\n",
    "        k=np.concatenate(ks_)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def visualize_hdbscan_plotly(pts: np.ndarray, outs: HDBSCANOutputs,\n",
    "                             show_shells=True, shell_opacity=0.28,\n",
    "                             shell_colors=None):\n",
    "    if shell_colors is None:\n",
    "        shell_colors = {60:\"#8c564b\", 80:\"#2ca02c\", 90:\"#ff7f0e\", 95:\"#1f77b4\"}\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # 1) points — one trace per label (noise, cluster 0, cluster 1, …)\n",
    "    for tr in _scatter_points_colored(pts, outs.labels, outs.probabilities):\n",
    "        tr.showlegend = True\n",
    "        fig.add_trace(tr)\n",
    "\n",
    "    # 2) shells — combine all clusters per quantile into one trace: q60/q80/q90/q95\n",
    "    if show_shells and outs.cluster_meshes:\n",
    "        # collect meshes by quantile\n",
    "        by_q: Dict[int, list[trimesh.Trimesh]] = {}\n",
    "        for _, qdict in outs.cluster_meshes.items():\n",
    "            for q, mesh in qdict.items():\n",
    "                by_q.setdefault(int(q), []).append(mesh)\n",
    "\n",
    "        # draw in outer→inner order for visibility\n",
    "        for q in sorted(by_q.keys()):  # 60,80,90,95\n",
    "            payload = _concat_meshes(by_q[q])\n",
    "            if payload is None:\n",
    "                continue\n",
    "            fig.add_trace(go.Mesh3d(\n",
    "                **payload,\n",
    "                name=f\"q{q}\",\n",
    "                opacity=shell_opacity,\n",
    "                color=shell_colors.get(int(q), \"#7f7f7f\"),\n",
    "                flatshading=True,\n",
    "                showscale=False,\n",
    "                showlegend=True\n",
    "            ))\n",
    "\n",
    "    # bounds & layout\n",
    "    mins, maxs = pts.min(0), pts.max(0)\n",
    "    ctr = (mins+maxs)/2; r = float((maxs-mins).max())/2\n",
    "    fig.update_layout(\n",
    "        width=1000, height=800,\n",
    "        scene=dict(\n",
    "            aspectmode=\"data\",\n",
    "            xaxis=dict(range=[ctr[0]-r, ctr[0]+r], title=\"X\"),\n",
    "            yaxis=dict(range=[ctr[1]-r, ctr[1]+r], title=\"Y\"),\n",
    "            zaxis=dict(range=[ctr[2]-r, ctr[2]+r], title=\"Z\"),\n",
    "        ),\n",
    "        legend=dict(itemsizing=\"constant\"),\n",
    "        margin=dict(l=0,r=0,t=30,b=0),\n",
    "        title=\"HDBSCAN Hotspots: clusters (color) + 60/80/90/95% shells\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "if __name__ == \"__main__\":\n",
    "    CSV = \"data/green_monkey/all_structure_files/chr1/12hrs/vacv/structure_12hrs_vacv_gene_info.csv\"\n",
    "    pts = load_points(csv=CSV)\n",
    "\n",
    "    cfg = HDBSCANConfig(\n",
    "        scale_coords=True,\n",
    "        min_cluster_size_strategy=\"auto\",          # auto over √N * [0.6..2.2]\n",
    "        min_cluster_size_grid=(0.6, 2.2, 9),\n",
    "        min_samples_options=(None, 1, 0.5, 1.0), # None=mcs; 1; 0.5*mcs; 1.0*mcs\n",
    "        cluster_selection_method=\"eom\",\n",
    "        metric=\"euclidean\",\n",
    "        make_shells=True,\n",
    "        shell_quantiles=(95, 90, 80, 60),\n",
    "        shell_sigma_vox=1.8,\n",
    "        shell_grid_base=96,\n",
    "        shell_margin=3.0,\n",
    "        out_dir=\"data/green_monkey/va_testing/hdbscan/chr1_12h_vacv\"\n",
    "    )\n",
    "\n",
    "\n",
    "    outs = run_hdbscan_hotspots(pts, cfg)\n",
    "\n",
    "    # Diagnostics (which params won)\n",
    "    print(outs.tuning_diagnostics.head(8))\n",
    "    print(\"\\nBest model:\",\n",
    "          \"min_cluster_size=\", outs.clusterer.min_cluster_size,\n",
    "          \"min_samples=\", outs.clusterer.min_samples,\n",
    "          \"n_clusters=\", len(set(outs.labels))- (1 if -1 in outs.labels else 0),\n",
    "          \"noise_frac=\", float(np.mean(outs.labels==-1)))\n",
    "\n",
    "    # Cluster summary table\n",
    "    print(\"\\nCluster summary:\")\n",
    "    print(outs.cluster_summary)\n",
    "\n",
    "    # Interactive view\n",
    "    # visualize_hdbscan_plotly(pts, outs, show_shells=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93bd42-55d7-48e5-801a-e0ff88e79712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
